{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3a7dec",
   "metadata": {},
   "source": [
    "__<span style=\"font-size: 40px;\">Capstone Project</span>__ \n",
    "\n",
    "__<span style=\"font-size: 40px;\">Auraly</span>__ \n",
    "\n",
    "\n",
    "__<font size=\"6\">1. Business Understanding</font>__   \n",
    "\n",
    "***<span style=\"font-size: 24px;\">1.1 Overview</span>***\n",
    "\n",
    "Music consumption today is highly personalized, with streaming platforms offering tailored recommendations based on listening habits. However, when it comes to emotional resonance, listeners still spend significant time manually curating playlists that match how they feel in the moment.\n",
    "The traditional approach of browsing by genre or artist fail to capture the subtle emotional layers that make a song resonate. This project will make discovering music more personalized and enjoyable for casual listeners, DJs, and streaming \n",
    "platform users.  \n",
    "\n",
    "\n",
    "***<span style=\"font-size: 24px;\">1.2 Stakeholder</span>*** \n",
    "\n",
    "1. *Music Listeners* – benefit from effortless mood based playlist creation and more emotionally resonant music discovery.\n",
    "2. *Streaming Platforms* – gain deeper user engagement and personalization features that differentiate their service.\n",
    "3. *DJs & Curators* – save time curating emotionally aligned sets for events or audiences.\n",
    "\n",
    "\n",
    "***<span style=\"font-size: 24px;\">1.3 Problem Statement</span>*** \n",
    "\n",
    "Even though music apps offer personalized recommendations, they still don’t understand how a listener feels. People often spend too much time searching for songs that match their mood because most platforms sort music by genre or artist, not emotion. This makes it hard to find the right songs quickly, and limits how personal and meaningful the listening experience can be.\n",
    "\n",
    "\n",
    "***<span style=\"font-size: 24px;\">1.4 Business Objective</span>*** \n",
    "\n",
    "To establish Auraly as an intelligent mood-based music classification system that enhances emotional connection and personalization in music streaming. By automating playlist creation through acoustic mood detection, Auraly aims to improve user engagement, simplify music curation, and unlock deeper, mood-driven discovery experiences across platforms.\n",
    "\n",
    "\n",
    "***<span style=\"font-size: 24px;\">1.5 Project Objectives</span>*** \n",
    "\n",
    "**Main Objective** \n",
    "\n",
    "To develop an intelligent music classification system that automatically identifies the emotional mood of songs using acoustic features, enabling more intuitive and personalized music experiences.\n",
    "\n",
    "**Specific Objectives** \n",
    "\n",
    "1. *Enable automated mood based playlist generation* - Reduce manual curation time by dynamically grouping songs based on emotional tone.\n",
    "2. *Support personalized music discovery* - Recommend songs that align with a listener’s current mood or emotional preferences.\n",
    "3. *Enhance user engagement across music platforms* - Improve retention and satisfaction by offering emotionally resonant listening experiences tailored to individual users.\n",
    "\n",
    "\n",
    "***<span style=\"font-size: 24px;\">1.6 Research Questions</span>*** \n",
    "\n",
    "1. How accurately can acoustic features be used to classify the emotional mood of a song?\n",
    "2. Does personalized mood-based music discovery lead to higher user engagement on streaming platforms?\n",
    "3. How can mood based classification improve the way users discover and organize music?\n",
    "\n",
    "\n",
    "***<span style=\"font-size: 24px;\">1.7 Success Criteria</span>***\n",
    "\n",
    "1. *Accurate Mood Classification* - The system achieves a high accuracy rate in classifying songs into predefined emotional categories based on acoustic features.\n",
    "2. *Improved User Experience* - Users will report reduced time and effort in creating mood based playlists and express higher satisfaction with music recommendations through surveys or usability testing.\n",
    "3. *Increased Engagement Metrics* - Streaming platforms or test environments will show measurable improvements in user engagement, e.g. longer listening sessions, more playlist saves, or higher interaction rates, when Auraly is integrated.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d175dc9",
   "metadata": {},
   "source": [
    "__<font size=\"6\">2. Data Understanding</font>__ \n",
    "\n",
    "---\n",
    "\n",
    "***<span style=\"font-size: 24px;\">2.1 Importing Relevant Libraries</span>***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a6cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a5b11",
   "metadata": {},
   "source": [
    "***<span style=\"font-size: 24px;\">2.2 Loading the Data</span>***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c61bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3735d39e",
   "metadata": {},
   "source": [
    "***<span style=\"font-size: 24px;\">2.3 Initial Exploration And EDA</span>***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84653839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f85a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "337e61a5",
   "metadata": {},
   "source": [
    "*<span style=\"font-size: 22px;\">2.3.2 Dataset summary</span>*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
